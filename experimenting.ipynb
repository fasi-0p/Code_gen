{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f438ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_code import Extract\n",
    "base_dir = r\"C:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\Data\"\n",
    "res=Extract.extract_all_functions(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "\n",
    "#chromaDB\n",
    "client = chromadb.PersistentClient(path=\"./code_index\")  #NEW\n",
    "collection = client.get_or_create_collection(\"code_snippets\")\n",
    "\n",
    "\n",
    "#ast of all languages\n",
    "from extract_code import Extract\n",
    "base_dir = r\"C:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\Data\"\n",
    "extracted=Extract.extract_all_functions(base_dir)\n",
    "\n",
    "# vectorizing and storing in chromaDB\n",
    "for idx, item in enumerate(extracted):\n",
    "    text_for_embedding = item[\"function_name\"] + \" \"+ item[\"code\"] + \" \" + item[\"docstring\"]\n",
    "    vector = get_embedding(text_for_embedding)\n",
    "\n",
    "    collection.add(\n",
    "        documents=[text_for_embedding],\n",
    "        embeddings=[vector],\n",
    "        metadatas=[{\n",
    "            \"function_name\": item[\"function_name\"],\n",
    "            \"file_path\": item[\"file_path\"],\n",
    "            \"language\": item[\"language\"],\n",
    "            \"docstring\": item[\"docstring\"]\n",
    "        }],\n",
    "        ids=[str(idx)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b132d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['104', '85', '112', '124', '136']], 'embeddings': None, 'documents': [['linearSearch  ', 'binarySearch  ', 'permute  ', 'sieve  ', 'find  ']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'file_path': 'C:\\\\Users\\\\FASI OWAIZ AHMED\\\\Desktop\\\\Code_gen\\\\Data\\\\Java\\\\linear search.java', 'function_name': 'linearSearch', 'docstring': '', 'language': 'java'}, {'file_path': 'C:\\\\Users\\\\FASI OWAIZ AHMED\\\\Desktop\\\\Code_gen\\\\Data\\\\Java\\\\binary search.java', 'language': 'java', 'function_name': 'binarySearch', 'docstring': ''}, {'language': 'java', 'file_path': 'C:\\\\Users\\\\FASI OWAIZ AHMED\\\\Desktop\\\\Code_gen\\\\Data\\\\Java\\\\permutations.java', 'docstring': '', 'function_name': 'permute'}, {'function_name': 'sieve', 'docstring': '', 'file_path': 'C:\\\\Users\\\\FASI OWAIZ AHMED\\\\Desktop\\\\Code_gen\\\\Data\\\\Java\\\\sieve of eratosthenes.java', 'language': 'java'}, {'docstring': '', 'function_name': 'find', 'file_path': 'C:\\\\Users\\\\FASI OWAIZ AHMED\\\\Desktop\\\\Code_gen\\\\Data\\\\Java\\\\union find disjoint set.java', 'language': 'java'}]], 'distances': [[28.193397521972656, 28.216230392456055, 31.987499237060547, 33.49070739746094, 33.84126281738281]]}\n"
     ]
    }
   ],
   "source": [
    "query_vector = get_embedding(\"find binary search in python\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],  # not query_texts\n",
    "    n_results=5\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a8ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "\n",
    "#chromaDB\n",
    "client = chromadb.PersistentClient(path=\"./code_index\")  # âœ… NEW\n",
    "collection = client.get_or_create_collection(\"code_snippets\")\n",
    "\n",
    "\n",
    "#ast of all languages\n",
    "from extract_code import Extract\n",
    "base_dir = r\"C:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\Data\"\n",
    "extracted=Extract.extract_all_functions(base_dir)\n",
    "\n",
    "\n",
    "# vectorizing and storing in chromaDB\n",
    "faiss_embeddings = []\n",
    "faiss_metadata = []\n",
    "\n",
    "for idx, item in enumerate(extracted):\n",
    "    text_for_embedding = item[\"function_name\"] + \" \"+ item[\"code\"] + \" \" + item[\"docstring\"]\n",
    "    vector = get_embedding(text_for_embedding)\n",
    "\n",
    "    collection.add(\n",
    "        documents=[text_for_embedding],\n",
    "        embeddings=[vector],\n",
    "        metadatas=[{\n",
    "            \"function_name\": item[\"function_name\"],\n",
    "            \"file_path\": item[\"file_path\"],\n",
    "            \"language\": item[\"language\"],\n",
    "            \"docstring\": item[\"docstring\"]\n",
    "        }],\n",
    "        ids=[str(idx)]\n",
    "    )\n",
    "    faiss_embeddings.append(vector)\n",
    "    faiss_metadata.append(item)\n",
    "    # Cleanup memory\n",
    "    del vector\n",
    "    torch.cuda.empty_cache()\n",
    "embedding_matrix = np.vstack(faiss_embeddings).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save FAISS index and metadata\n",
    "faiss.write_index(index, \"code_index.faiss\")\n",
    "with open(\"faiss_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(faiss_metadata, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b79e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\FASI OWAIZ AHMED\\Desktop\\Code_gen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load everything\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index = faiss.read_index(\"code_index.faiss\")\n",
    "with open(\"faiss_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Embed query\n",
    "query = \"find binary search in python\"\n",
    "query_vector = get_embedding(query).reshape(1, -1).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_vector, 5)\n",
    "\n",
    "# Show results\n",
    "for i in I[0]:\n",
    "    print(metadata[i][\"function_name\"], \"->\", metadata[i][\"file_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
